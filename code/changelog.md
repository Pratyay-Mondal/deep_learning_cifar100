# Task 1: Complete Changelog

## Summary of Fixes
This document records the tabular listing of all the code fixes in the codebase. All bugs identified in `data_loader.py`, `models.py`, `train.py`, and `utils.py` are detailed below.

| Commit SHA | File & Approx. Line | Problem Description | Root Cause / Explanation | Introduced Change | Discovery Method |
| :--- | :--- | :--- | :--- | :--- | :--- |
| `[cac7cff2e770b4aa7731f4f7632602f9a3e8a845]` | `data_loader.py` (L25) | **Incorrect Normalization** | Negative means and very tiny standard deviation were incorrect | Updated `mean` and `std` to correct CIFAR-10 statistics. | **Data Inspection:** Ran `__main__` block; observed massive min/max values. |
| `[31c221064c46f9db35e8e4989b43338bcdf21380]` | `models.py` (MLP Class-L14) | **Runtime Error:** MLP input size shape Mismatch. | MLP Input layer was defined as `1024` (assuming Grayscale) instead of `3072` (3x32x32 for CIFAR-10). | Changed `input_size` default from 1024 to 3072. | **Manual Calculation:** Calculated 3x32x32 = 3072 and compared to code. |
| `[b7cf1b5b37b83e25cf9440e8a3c05c1f89a8925b]` | `models.py` (CNN Class-L67) | **Runtime Error:** CNN Fully connected layer input size shape Mismatch. | The first Linear layer expected `1024` inputs, but the CNN output flattened to `4096` (64 channels x 8x8). | Changed `nn.Linear(1024)` to `nn.Linear(4096)`. | **Shape Tracing:** Traced tensor dimentions through Convolutional/Poolling layers manually (32->16->8). |
| `[4b87433899e95ae89010b573d0d7426187e05603]` | `models.py` (MLP Class-L28) | **Vanishing Gradients** / Poor and bad Learning. | `ReLU` activation was placed at the very end of the network, causing zeroing out negative logits. | Removed the final `nn.ReLU()`. | **Architecture Review:** Checked against `CrossEntropyLoss` requirements (needs raw logits). |
| `[0bcf30989974ec863a0f80afc9a0d03900eae5b4]` | `models.py` (CNN Class-L69) | **Linear Layer Collapse.** | Two `Linear` layers in the CNN class were together without a non-linearity between them. | Added `nn.ReLU()` between the two fully connected layers. | **Code Review:** Spotted `Linear -> BatchNorm -> Linear` pattern missing an activation function. |
| `[a9d941df2326e79f14538de5c3a615f0eec5ee93]` | `train.py` (L74) | **Incorrect Loss Calculation.** | Code used `NLLLoss` (needs LogProbs) but model output raw logits. | Changed criterion to `nn.CrossEntropyLoss()`. | **Theoretical Analysis:** Verified `NLLLoss` requires `LogSoftmax`, which was missing. |
| `[5fa988067a0d5f20b1e7ce65f3a25ed4f32c701d]` | `train.py` (L146) | **Invalid Test Metrics.** | The final test evaluation was running on `train_loader`, giving false high accuracy. | Changed argument from `train_loader` to `test_loader`. | **Variable Inspection:** Noticed wrong variable name is passing to `evaluate`. |
| `[841e4468f28d54968c875ade4261b9bae03729c2]` | `utils.py` (L34) | **Exploding Gradients** . | `optimizer.zero_grad()` was missing. Gradients accumulated indefinitely. | Added `optimizer.zero_grad()` before backward pass. | **Loss Curve Analysis:** Loss did not decrease and quickly became massive. |
| `[c8e6521ed0da81f210a4e0376360a598882df0fc]` | `utils.py` (L25) | **Dropout/BatchNorm Inactive.** | The model was missing the training mode function. | Added `model.train()` at start of function. | **Code Review:** Noticed `model.eval()` in evaluation but missing `model.train()` in training loop. |
